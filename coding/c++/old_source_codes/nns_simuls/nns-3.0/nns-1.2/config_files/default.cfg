# default-tdgo-2x2.cfg
#
# Reinforcement Learning (RL) for computer Go.
# The board size is 2x2 in this case.
#
# Refer to p.149 in Sutton & Barto book.
#
# Last updated: Apr. 6, 2011 (Wed)
# First written: Apr. 6, 2011 (Wed)
#
# Examples:
#   No space is allowed a in variable name
#     simul_task_id = 2
#     pso_p_best_initial_value = inf
# Do not use ";" at the end of the line.
#   Examples. The following lines are wrong.
#     simul_task_id = 2;
#     pso_p_best_initial_value = inf;
#
#   For a boolen type variable of which option is limited only to on or off,
# do not use "true" or "false", rather use 0 or 1 unless it's specified.
#
# Refer to the manual for details.

################################################################################
#                           ConfigSimulations                                  #
################################################################################
# Options for possible variables
#
# simul_task_type: 
#  cmlp_string_count, learn_optimal_moves, plot_act_fn_mod_3, test_ter2bin,
#  test_cliff_walking, td_go
#    Use "learn_optimal_moves" to train CNN for the Go data.
#    Note: if simul_task_type = plot_act_fn_mod_3, most of the options are disabled.
# simul_boardSize: 2
#   The program is designed only for 2 now.
# simul_trace_all: 0 (off), 1 (on).        Typicall, this is turned off.
# simul_trace_on: 0 (off), 1 (on).         Typicall, this is turned off.
# simul_postprocess: 0 (off), 1 (on).      Typicall, this is turned off.
# simul_show_progress_bar: 0 (off), 1 (on) Typicall, this is turned on.
# simul_player: black, white
#   The input data is separated for black and white Go player.
#   The CNN should be trained separately for each player.
# simul_data_type: single_valued_function, multi_valued_function
# simul_trace4debug: 0 (off), 1 (on).      Typicall, this is turned off.
#   Turn on/off some debugging information into the trace file.

simul_task_type         = td_go
simul_boardSize         = 2
simul_show_progress_bar = 1
simul_trace4debug       = 0

# Design issue: simul_boardSize & simul_player may be in a new class ConfigBaduk.
# Leave them because there aren't many variables to configure the Baduk games now.

################################################################################
#                               ConfigData                                     #
################################################################################
# About input, output, target data
# test_problem-1:
#   x = [-1.0:0.1:1.0], y = 2.0*x^2-1.0;
# Note for "y = 2.0*x^2+1.0;", there is no way for NNs output to reach y=3.0
# because y_hat is bound to [-1.0, 1.0].
#
#data_input_config         = 2:2:16
#data_input_config         = 2:2:7
# Currently, this is not neccessary.

################################################################################
#                      ConfigReinforcementLearning                             #
################################################################################
# Options for possible variables
#
# file_gnuplot_input
#    a text file with data to plot. Use .dat for file extension.
# file_gnuplot_config
#
# rl_move_option: standard_move, kings_move, kings_move_with_pause
#   This is only for a test problem such as cliff walking and windy grid world.
#
# rl_update_method: sarsa, qlearning
# rl_move_option: greedy, egreedy
#
# rl_reward_structure: in double
#   Rewards of different cases are delimited by column ":".
#   Examples
#     -1.0:-100.0 = "roaming around":"is cliff".

rl_update_method             = qlearning
rl_action_criterion          = egreedy

rl_max_episode               = 2
rl_reward_structure          = 1.0:0.0:-1.0
rl_alpha                     = 0.1
rl_gamma                     = 1.2
rl_epsilon                   = 0.1
