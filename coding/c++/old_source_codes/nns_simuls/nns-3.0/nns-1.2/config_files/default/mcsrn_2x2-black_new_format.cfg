# mcsrn_wt_idr_2x2.cfg
#
# Modified CSRN (MCSRN) with input dimension reduction for a 2x2 board.
# The multivalued function is transformed into a single-valued function.
#
# Last updated: Mar. 4, 2011 (Fri)
# First written: May 25, 2009 (Mon)
#
# Examples:
#   No space is allowed a in variable name
#     simul_task_id = 2
#     pso_p_best_initial_value = inf
# Do not use ";" at the end of the line.
#   Examples. The following lines are wrong.
#     simul_task_id = 2;
#     pso_p_best_initial_value = inf;
#
#   For a boolen type variable of which option is limited only to on or off,
# do not use "true" or "false", rather use 0 or 1 unless it's specified.
#
# Refer to the manual for details.
################################################################################
#                            ConfigDirStruct                                   #
################################################################################
# Default values
#
# dir_outputs             = outputs
# dir_gnuplot_config_file = gnuplot_config_files
# file_trace              = default.trc
# file_error_profile      = error.prf
# file_nn_weights         = nnets_weights.cnn
# file_nn_init_weights    = initial_weights.cnn
#
# Note "file_nn_weights" is a file for neural networks weights.
#
# Options for possible variables
#
# file_gnuplot_input
#    a text file with data to plot. Use .dat for file extension.
# file_gnuplot_config
#    File extension is ".gp" meaning g(nu)p(lot).
# file_gnuplot_output
#    File extension is ".ps" or p(ost)s(cript).
#    Ensure gnuplot_"file_gnuplot_output" matches the output file name in 
#    "file_gnuplot_config."
#
# Assumption
#    1. The directores specified by this program have already been made.
#    2. File names except trace files should exist in the designated locations.
# Default values: directory structure and file names
#    The default directory structure and file names are set in the source code,
#    i.e. void Myiostream::setDefaultValues(). Class ConfigDirStruct provides
#    flexibility to change the directory & file names in the configuration file.
# cwd (Current Working Directory)
#    cwd is displayed in the terminal when the program is run. 
# Directory for a config file: directory config_files
#    It should be hard-coded in the source code or a config file cannot be read.

file_nn_init_weights = weights0121_2011.cnn
file_gnuplot_input   = gnuplot_test_input.dat
file_gnuplot_config  = gnuplot_default_config.gp
file_gnuplot_output  = gnuplot_test_output.ps

################################################################################
#                               ConfigData                                     #
################################################################################
# About input, output, target data
# test_problem-1:
#   x = [-1.0:0.1:1.0], y = 2.0*x^2-1.0;
# Note for "y = 2.0*x^2+1.0;", there is no way for NNs output to reach y=3.0
# because y_hat is bound to [-1.0, 1.0].
#
# Options for possible variables
#
# data_input_config: e.g. 2:2:16
# data_target_config: 

# data_representation: ternary
# data_representation_offset: 0.0, 1.0
# data_type: board_status_sequences, single_valued_function, multi_valued_function

data_representation        = ternary
data_representation_offset = 1.0
data_type                  = board_status_sequences

################################################################################
#                           ConfigSimulations                                  #
################################################################################
# Options for possible variables
#
# simul_task_type: cmlp_string_count, learn_optimal_moves, plot_act_fn_mod_3, test_ter2bin
#    Use "learn_optimal_moves" to train CNN for the Go data.
#    Note: if simul_task_type = plot_act_fn_mod_3, most of the options are disabled.
# simul_boardSize: 2
#   The program is designed only for 2 now.
# simul_trace_all: 0 (off), 1 (on).        Typicall, this is turned off.
# simul_trace_on: 0 (off), 1 (on).         Typicall, this is turned off.
# simul_postprocess: 0 (off), 1 (on).      Typicall, this is turned off.
# simul_show_progress_bar: 0 (off), 1 (on) Typicall, this is turned on.
# simul_player: black, white
#   The input data is separated for black and white Go player.
#   The CNN should be trained separately for each player.
# simul_data_type: single_valued_function, multi_valued_function

simul_task_type           = learn_optimal_moves
simul_postprocess         = 0
simul_show_progress_bar   = 1
simul_boardSize           = 2
simul_player              = black

# Design issue: simul_boardSize & simul_player may be in a new class ConfigBaduk.
# Leave them because there aren't many variables to configure the Baduk games now.

################################################################################
#                               ConfigCnn                                      #
################################################################################
# Options for possible variables
#
# cnn_nn_type: cmlp, cpsrn, csrn, mcsrn
#    mcsrn automatically uses the input dimension reduction technique.
# cnn_max_epoch_o: any positive integer number
#    The maximum number of epochs for the outer loop
# cnn_max_epoch_i: any positive integer number
#    The maximum number of epochs for the inner loop
# cnn_target_system_error: any non-negative real value
#    Target error for the outer loop.
# cnn_target_cell_error: any non-negative real value
#    Target error for the inner loop within cells.
#    Set this value to "-1.0" so the soft hibernation problem can be avoided.
# cnn_fd_input_initialization: zeros, ones, random, target,rand_ternary
#    CMLP does not use this parameter. It is for cnn_nn_type with recurrence or
#    feedback input(s).
#    - zeros
#      is not much different from random. After the first internal loop, the
#      output is computed by the neural networks' weights. If the neural networks
#      weights are randomized, the first neural networks output is more or like
#      random.
#    - target
#      is to set the initial values to the target values.
#    - ones
#      This is my favorite initialization method so far.
# cnn_error_metric: mse, abs_error
#    mse (Mean Square Error)
#    abs_error (Absolute Error) error=abs(target data-estimated data)
# cnn_training_algorithm: pso
# cnn_match_init_weights_to_training_algo: 0 (off), 1 (on)
#       Say CNN's initial weights are randomized. These weights are matched to the 
#    training algorithm. For example, PSO's initial particle locations are set by
#    CNN's corresponding weights. 
# neuralnets_initial_weights_mode: random, load_manually
#    A variable "file_nn_weights" is important for both options.
#       For example, "file_nn_weights = weights.cnn". 
#    random: the randomized initial weights are saved in "file_nn_weights".
#               The usefulness of this feature is questionable at first sight 
#            because PSO randomizes CNN's weights anyways. There is a fundamental 
#            difference here, though. If we let PSO randomizes CNN's initial 
#            weights, what actually happens is PSO selects the best particle's 
#            position as initial weights. 
#               Stricly speaking, this initial weights are ones after one epoch 
#            of training. Letting PSO initialize the weights is not random 
#            strictly speaking. Therefore, CNN should allow the random CNN weights
#            overload the initial PSO's best particle's position. Namely CNN should
#            not let PSO overtake the CNN's initialization. Otherwise, PSO's 
#            best particle's position after one epoch will mistakenly initialize
#            CNN's weights.
#    load_manually: the initial weights in "file_nn_weights" are loaded as the
#       weights in class NeuralNetworks. Therefore CNN can start from the 
#       same weights with this option. This feature is incorporated to repeat 
#       experiments with identical initial weights.
#          CNN's internal behavior is so strongly interwound that understanding the 
#       behavior is complex. Therefore, it is often neccessary to start from identical 
#       initial weights in order to reduce the dynamics of starting from different 
#       initial weights at each run of simulations.

cnn_nn_type                             = mcsrn
cnn_training_algorithm                  = pso
cnn_error_metric                        = abs_error
cnn_target_system_error                 = 0.0
cnn_target_cell_error                   = -1.0
cnn_fd_input_initialization             = ones
cnn_max_epoch_o                         = 10000
cnn_max_epoch_i                         = 500
cnn_match_init_weights_to_training_algo = 1
cnn_initial_weights_mode                = load_manually

################################################################################
#                          ConfigNeuralNetworks                                #
################################################################################
# Options for possible variables
#
# neuralnets_number_of_neurons_in_layers: any non-negative integer separated by colon ":".
#    For example, "neuralnets_number_of_neurons_in_layers = 6:1". "6:1" means 
#    N input layer nodes, 6 hidden layer neurons & 1 output neuron.
#    The number of input layer nodes is supposed to be determined automatically
#    by the input data size. Read below for further explanation.
#      It supports any number of neurons with any number of layers because this 
#    is implemented with a container vector. Related variables such as
#    the number of hidden layers are automatically computed, too.
#      Note "neuralnets_number_of_nodes_in_layers" is obsolete because the number
#    of input layer nodes may vary in the CNN architecture. An example for this
#    is "neuralnets_number_of_nodes_in_layers = 6:6:1". 6 input layer nodes,
#    6 hidden layer neurons & 1 output neuron.
#
#    Be careful not to include the number of input nodes because it will be 
#    determined by other factors such as the board size and a cell structure, 
#    e.g. MLP or SRN.
#      For example, MCSRN for the 2x2 uses the input dimension reduction. 
#    The number of nodes in the input layer is 3, so the the neural networks
#    architecture with "neuralnets_number_of_neurons_in_layers = 2:1" becomes
#    "3:2:1". Be careful not to set "neuralnets_number_of_neurons_in_layers" to
#    "neuralnets_number_of_neurons_in_layers = 3:2:1". This setting is wrong
#    because the hidden layer has three neurons and the output layer has two
#    neurons.
#
#      Note a node is a super set of a neuron. A neuron is an elementary 
#    computational unit. For example, a neuron performs linear combination
#    followed by an activation function. A node is a point in a graph model
#    to depict a neural network. When the point can be a neuron or just a point
#    to bypass a data. The input nodes do not perform any computation, rather
#    they bypass the given data. 
# neuralnets_activation_function: linear, tansig, logsig, floor, ternary_step, mod_3
#       output layer activation fn    activation fn for the rest of them
#   1            linear                           tansig
#   2            linear                           logsig
#   3            tansig                           tansig
#   4            logsig                           logsig
#   5            floor                            tansig 
#   6            ternary_step                     tansig
#   7            ternary_step                     logsig
#   8            mod_3                            tansig
#   9            mod_3                            logsig
#   Range of the activation output
#     tansig: a real value in [-1,1] 
#     logsig: a real value in [0,1]
#     ternary_step: an integer value in [0,2]
#     mod_3: an integer value in [0,2]
# neuralnets_activation_function_hidden_layer: linear, tansig, logsig, floor, ternary_step, mod_3
#   This substitutes "neuralnets_activation_function" and manually set activation 
#   function in the hidden layer(s).
# neuralnets_activation_function_output_layer: linear, tansig, logsig, floor, ternary_step, mod_3
#   This substitutes "neuralnets_activation_function" and manually set activation 
#   function in the output layer.
# neuralnets_slope_param_logsig: any real number
#    For example, "neuralnets_slope_param_logsig = 1.0".
# neuralnets_slope_param_tansig: any real number
#    For example, "neuralnets_slope_param_tansig = 1.0".
#
# Variables only for ternary_step activation function
#   c is the center of the step function and w is the width.
# neuralnets_ternary_step_function_c: any real number
#    For example, "neuralnets_ternary_step_function_c = 10.0"
# neuralnets_ternary_step_function_w: any real number
#    For example, "neuralnets_ternary_step_function_c = 10.0"

neuralnets_number_of_neurons_in_layers      = 7:1
neuralnets_activation_function_hidden_layer = tansig
neuralnets_activation_function_output_layer = mod_3
neuralnets_slope_param_logsig               = 1.0
neuralnets_slope_param_tansig               = 1.0

################################################################################
#                               ConfigPso                                      #
################################################################################
# These parameters configures the PSO particles' behavior in each cell. Note
# an instance of class Pso resides in a cell.
#
# Options for possible variables
#
# pso_number_of_particles: any non-negative integer value, e.g. 50
#    Be careful not to set "pso_number_of_particles=1".
# pso_w: any real value between 0.0 and 1.0, e.g. 0.2
# pso_c1: any real value, in the original PSO paper, this value is set to 2.0
# pso_c2: any real value, in the original PSO paper, this value is set to 2.0
# pso_v_max: any real value, e.g. 2.0
# pso_x_min: any real value, e.g. -100.0
# pso_x_max: any real value, e.g. 100.0
# pso_v_initial_value: any real value, e.g. 0.000000000000001
#   Don't set it to zero or the initial V_[i] to zero. When the initial velocity
#   is set to zero, the velocity will be frozen to zero all the time because
#   the globally best particle stops moving around.
#      If you don't like a large value, pick a small value around zero.
# pso_global_best_initial_value: inf, cnn, or any floating number.
#    When pso_global_best_initial_value=inf, the first particle's position becomes
#    the global best's position. So pso_global_best_initial_value should be inf 
#    in order for PSO to work properly. Note, however, this setting is for the 
#    conventional neural networks.
#
# pso_use_particle_history_reset: 0 (off), 1 (on)
#      This is a new feature for our novel PSO algorithm or RPSO (Recurrent PSO).
#    The history or the particle best for non-best particles is reset/reinitialized.
#    Note a multiple number of particles can have the same fitness. In this case,
#    the history of all these particles are kept. That of the rest of particles is
#    reset. 

pso_number_of_particles        = 1000
pso_w                          = 0.10
pso_c1                         = 1.0
pso_c2                         = 1.0
pso_v_max                      = 1.0
pso_x_min                      = -500.0
pso_x_max                      = 500.0
pso_v_initial_value            = 0.000000000000001

pso_use_particle_history_reset = 1

################################################################################
#                             ConfigGnuPlot                                    #
################################################################################
# Options for possible variables
#
# gnuplot_run: 0 (off), 1 (on)
#    Setting it to 1 automatically launches GNU plot after running a simulation.
# gnuplot_title
# gnuplot_range_x_from
# gnuplot_range_x_to
# gnuplot_range_x_increment

gnuplot_run               = 0
gnuplot_title             = my_first_plot
gnuplot_range_x_from      = -3.0
gnuplot_range_x_to        =  3.0
gnuplot_range_x_increment =  0.5
