{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 개요 (Overview)\n",
    "\n",
    "* Last updated: 2019-10-16 (Wed)\n",
    "* First written: 2019-10-16 (Wed)\n",
    "* Tae-Hyung Kim, Ph.D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 목표 (Objectives)\n",
    "* 인공지능과 자연어처리 (Natural Language Processing)를 공부하기 위해서\n",
    "  * 텍스트 처리 (Text Processing)의 한 분야인 감성 분석 (Sentiment Analysis) 문제를 다뤄봅니다.\n",
    "  * 감성 분석을 수행하는 코드를 이해하고 결과를 재생산 (Reproduce)하기 위해 필요한 지식을 습득합니다.\n",
    "  * 필요한 지식을 이해하기 위한 배경지식을 습득합니다.\n",
    "* 이 과정에서\n",
    "  * 인공지능의 한 분야인 \n",
    "    * 기계학습 (혹은 머신러닝 Machine Learning)과\n",
    "    * 심층학습 (혹은 딥러닝 Deep Learning)\n",
    "  * 의 개념과 일부 기술을 이해하고 필요한 기술을 습득합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 결과 예시: 소스 코드 샘플 (Source Code Sample)\n",
    "아래의 소스 코드는\n",
    "* Keras로 IMDB Movie Review Dataset에 대한 감성 분석을 하는 간단한 예제 코드입니다.\n",
    "* RNN의 한 종류인 LSTM으로 데이터셋을 훈련 (Training)하고 테스트 (Test) 합니다.\n",
    "* 학습된 딥러닝 모델로 유사한 Movie Review Data에 대한 결과 예측 (Prediction)이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#%%##################\n",
    "# Directory & Files #\n",
    "#####################\n",
    "dir_data_in        = '../data_in/'\n",
    "dir_data_out       = '../data_out/'\n",
    "dir_checkpoint_rnn = '../checkpoint/rnn'\n",
    "\n",
    "if not os.path.exists( dir_data_out ):\n",
    "    os.makerdirs( dir_data_out )\n",
    "\n",
    "# The input & label data were saved to a Numpy file.\n",
    "file_train_input_data = 'train_input.npy'\n",
    "file_train_label_data = 'train_label.npy'\n",
    "file_test_input_data  = 'test_input.npy'\n",
    "file_test_id_data     = 'test_id.npy'\n",
    "\n",
    "file_data_configs     = 'data_configs.json'\n",
    "file_result_output    = 'movie_review_result-rnn.csv'\n",
    "#%%##############\n",
    "# Load the data #\n",
    "#################\n",
    "# The preprocessed data was saved to numpy files (binary)\n",
    "file = dir_data_out + file_train_input_data\n",
    "with open( file, 'rb') as f:\n",
    "    input_data = np.load( f )\n",
    "    print(f'Loaded from {file}...')\n",
    "\n",
    "file = dir_data_out + file_train_label_data\n",
    "with open( file, 'rb') as f:\n",
    "    label_data = np.load( f )\n",
    "    print(f'Loaded from {file}...')\n",
    "\n",
    "# The vocaburary and stuff...\n",
    "#prepro_configs = None\n",
    "file = dir_data_out + file_data_configs\n",
    "with open( file, 'r') as f:\n",
    "    prepro_configs = json.load( f )\n",
    "\n",
    "#>>> prepro_configs\n",
    "#'data_configs.json'\n",
    "\n",
    "#%%###############\n",
    "# Configurations #\n",
    "##################\n",
    "random_seed      = 13371447\n",
    "test_split_ratio = 0.1  # TODO:Why is this changed to 0.1 from 0.2?\n",
    "batch_size       = 16\n",
    "num_epochs       = 3\n",
    "\n",
    "#%%###################\n",
    "# Training the Model #\n",
    "######################\n",
    "# Split training and testing data\n",
    "train_input, test_input, train_label, test_label = train_test_split( input_data, label_data, test_size=test_split_ratio, random_state=random_seed )\n",
    "# train_input, eval_input, train_label, eval_label = train_input, test_input, train_label, test_label\n",
    "# train_data_features, y = input_data, label_data\n",
    "\n",
    "def mapping_fn(x, y):\n",
    "    inputs, labels = {'x': x}, y\n",
    "    return inputs, labels\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset  = tf.data.Dataset.from_tensor_slices( (train_input, train_label) )\n",
    "    dataset  = tf.data.shuffle( buffer_size=50000 )\n",
    "    dataset  = tf.data.batch( batch_size )\n",
    "    dataset  = tf.data.repeat( count=num_epochs )\n",
    "    dataset  = tf.data.map( mapping_fn )\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset  = tf.data.Dataset.from_tensor_slices( (test_input, train_label) )\n",
    "    dataset  = tf.data.batch( batch_size )\n",
    "    dataset  = tf.data.map( mapping_fn )\n",
    "    #dataset  = tf.data.batch( batch_size*2 )  # TODO: why *2  and AFTER map?\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "#%%###################\n",
    "# Training the Model #\n",
    "######################\n",
    "#print( type( prepro_configs['vocab_size'] ) )\n",
    "print( int( prepro_configs['vocab_size'] ) )\n",
    "\n",
    "vocab_size        = prepro_configs['vocab_size'] + 1  # +1 is for 'unk' or unknown\n",
    "# TypeError: string indices must be integers\n",
    "\n",
    "word_embeding_dim = 100\n",
    "hidden_state_dim  = 150\n",
    "dense_feature_dim = 150\n",
    "drop_out_rate     = 0.2\n",
    "learning_rate     = 0.001\n",
    "\n",
    "# Double check the result\n",
    "print( len(prepro_configs['vocab']), vocab_size )\n",
    "# TODO: assert?\n",
    "\n",
    "def model_fn( features, labels, mode ):\n",
    "    # True if the condition is met\n",
    "    training_mode   = ( mode == tf.estimator.ModeKeys.TRAIN )    \n",
    "    testing_mode    = ( mode == tf.estimator.ModeKeys.EVAL )\n",
    "    prediction_mode = ( mode == tf.estimator.ModeKeys.PREDICT )\n",
    "    \n",
    "    input_data      = features['x']\n",
    "    embedding_layer = tf.keras.layers.Embedding( vocab_size, word_embeding_dim)( input_data )\n",
    "    rnn_layers      = [tf.nn.rnn_cell.LSTMCell(size) for size in [hidden_state_dim, hidden_state_dim]]\n",
    "    multi_rnn_cell  = tf.nn.rnn_cell.MultiRNNCell( rnn_layers )\n",
    "    outputs, state  = tf.nn.dynamic_rnn( cell=multi_rnn_cell,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         dtype=tf.float32 )\n",
    "    \n",
    "    outputs          = tf.keras.layers.Dropout( drop_out_rate )( outputs )\n",
    "    hidden_layer     = tf.keras.layers.Dense( dense_feature_dim,\n",
    "                                              activation=tf.nn.tanh )( outputs[:,-1,:] )\n",
    "    hidden_layer     = tf.keras.layers.Dropout( drop_out_rate )( hidden_layer )\n",
    "    \n",
    "    logits           = tf.keras.layers.Dense(1)( hidden_layer )\n",
    "    logits           = tf.squeeze( logits, axis=-1 )\n",
    "    sigmoid_logits   = tf.nn.sigmoid( logits )\n",
    "    \n",
    "    # TODO: I have to double-check if logits are the right one to use!\n",
    "    loss             = tf.losses.sigmoid_cross_entropy( labels, logits )\n",
    "    \n",
    "    if training_mode:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op    = tf.train.AdamOptimizer( learning_rate ).minimize( loss, global_step )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec( mode=mode, train_opt=train_op, loss=loss )\n",
    "    \n",
    "    if testing_mode:\n",
    "        classes         = tf.round( sigmoid_logits)\n",
    "        accuracy        = tf.metrics.accuracy( labels, classes )\n",
    "        eval_metric_ops = {'acc': accuracy}  # TODO: or acc-> accuracy\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops = eval_metric_ops )\n",
    "    \n",
    "    if prediction_mode:\n",
    "        predictions = {'sentiment': sigmoid_logits }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec( mode=mode, predictions=predictions )\n",
    "\n",
    "#%%#################\n",
    "# Train & Evaluate #\n",
    "####################\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "\n",
    "directory = dir_data_out + dir_checkpoint_rnn\n",
    "est = tf.estimator.Estimator( model_fn=model_fn, model_dir=directory)\n",
    "est.train( train_input_fn )\n",
    "est.evaluate( eval_input_fn )\n",
    "\n",
    "#%%##############\n",
    "# Load the data #\n",
    "#################\n",
    "# The preprocessed data was saved to numpy files (binary)\n",
    "file = dir_data_out + file_test_input_data\n",
    "with open( file, 'rb') as f:\n",
    "    test_input_data = np.load( f )\n",
    "    print(f'Loaded from {file}...')\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn( x={'x':test_input_data}, shuffle=False )\n",
    "\n",
    "predictions = np.array( [p['sentiment'] for p in est.predict( input_fn=predict_input_fn)] )\n",
    "\n",
    "#%%##################\n",
    "# Saving the Result #\n",
    "#####################\n",
    "\n",
    "file = dir_data_out + file_test_id_data\n",
    "with open( file, 'rb') as f:\n",
    "    test_id = np.load( f )\n",
    "    print(f'Loaded from {file}...')\n",
    "\n",
    "# From dictionary to dataframe\n",
    "result    = {'id': list(test_id), 'sentiment': list(predictions) }\n",
    "result_df = pd.DataFrame( result )\n",
    "\n",
    "file = dir_data_out + file_result_output\n",
    "result_df.to_csv( file, index=False, quoting=3 )\n",
    "print(f'Saved to {file}...')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 학습 이전에 필요한 절차들...\n",
    "위의 코드는 훈련 데이터와 테스트 데이터가 주어진 상황에서 훈련과 테스트를 수행합니다. 그 이전에 수행되어야 할 데이터 준비 (Data Preparation) 과정들이 포함되지 않았습니다. 예를 들면, 아래의 절차는 생략했습니다.\n",
    "* 탐색적 데이터 분석 (Exploratory Data Analysis or EDA)\n",
    "* 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "하지만 이런 과정은 훈련 및 테스트 데이터를 생성하기 위해 필요합니다. 이에 더해 딥러닝 모델의 성능은 \n",
    "1. 적용되는 딥러닝 기술 (예: LSTM)과\n",
    "2. 어떤 데이터셋으로 훈련하는지\n",
    "\n",
    "에 의해 좌우됩니다. 그러므로 데이터 준비 과정을 위한 기본적인 기술을 이해하고 적용할 수 있는 능력은 의미가 있습니다.\n",
    "\n",
    "왜냐하면 장난처럼 작은 문제 (Toy Problem)나 교실 안 문제 (Class room Problem)는 데이터가 주어지지만 실제로 프로젝트를 수행할 때는 (Real World Problem) 스스로 데이터를 준비해야 하기 때문에 데이터 준비 과정에 대한 이해는 딥러닝을 수행하는데 있어 필수적인 기술입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. 리뷰할 소스코드\n",
    "이런 맥락에서 하나의 딥러닝 기법인 LSTM으로 감성분석 과제(Task)를 수행하는 모델을 만들기 위한 학습 과정에 있는 핵심 절차와 내용을 정리해봅니다. 참고로 감성분석은 텍스트 처리 (혹은 자연어 처리) 응용분야 중 하나입니다. 이 과제에 대한 이해를 확장하여 다른 응용분야의 과제들을 하나씩 도전하다보면, 텍스트 처리 (혹은 자연어 처리) 분야에 대한 전반적인 이해가 가능합니다. 이에 더해 머신러닝/딥러닝 모델의 코드는 재사용 (Code Reuse)이 가능합니다. 분야를 텍스트에서 이미지 (Image)나 음성 (Speech)에 확장할 때는 데이터 준비 및 인코딩 과정이 바뀝니다.\n",
    "\n",
    "### 데이터 생성 (Data Generation)\n",
    "* 1_1-exploratory_data_analysis-imdb_movie_review_dataset.py\n",
    "* 1_2-data_preprocessing-imdb_movie_review_dataset.py\n",
    "\n",
    "### 데이터 인코딩 (Data Encoding), i.e. 텍스트 인코딩 (Text Encoding)\n",
    "* 2_1-vectorizing_text_with_tf-imdb_movie_review_dataset.py\n",
    "  * from sklearn.feature_extraction.text import CountVectorizer\n",
    "* 2_2-vectorizing_text_with_tf_idf-imdb_movie_review_dataset.py\n",
    "  * from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "* 2_3-vectorizing_text_with_word_embedding-imdb_movie_review_dataset.py\n",
    "\n",
    "TODO: \n",
    "1. Split 3-vectorizing_text_with_tf_idf-imdb_movie_review_dataset.py into two:\n",
    "  * 4-vectorizing_text_with_tf_idf-imdb_movie_review_dataset.py and\n",
    "  * 6-modeling_with_logistic_regression-imdb_movie_review_dataset.py\n",
    "2. Extract a part of code to make \"3-vectorizing_text_with_tf-imdb_movie_review_dataset.py\"\n",
    "\n",
    "### 모델 훈련 및 평가 (Training and Evaluation of Models)\n",
    "#### 머신러닝 (Machine Learning)\n",
    "* 3_1-modeling_with_logistic_regression-imdb_movie_review_dataset.py\n",
    "* 3_2-modeling_with_random_forest-imdb_movie_review_dataset.py\n",
    "\n",
    "#### 딥러닝 (Deep Learning)\n",
    "* 3-modeling_with_rnn-imdb_movie_review_dataset.py\n",
    "* modeling_with_cnn-imdb_movie_review_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(EOF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
