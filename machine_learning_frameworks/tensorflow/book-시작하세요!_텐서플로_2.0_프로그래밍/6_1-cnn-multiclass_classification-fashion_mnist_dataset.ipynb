{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1. Multiclass Classification with CNN for the Fashion MNIST Dataset\n",
    "* \"시작하세요! 텐서플로우 2.0 프로그래밍\",\n",
    "  * 6.3. Fashion MNIST 데이터세트에 적용하기, pp.150-161\n",
    "  * 6.4. 퍼포먼스 높이기, pp.161-172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_an_image( numpy_ndarray, cmap=None ):\n",
    "  # Fashion MNIST Dataset\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  plt.imshow( numpy_ndarray, cmap )\n",
    "  plt.colorbar()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist_module = tf.keras.datasets.fashion_mnist\n",
    "(train_x, train_y), (test_x, test_y) = fashion_mnist_module.load_data()\n",
    "\n",
    "show_an_image( train_x[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN에 쓰이는 Conv2D 레이어는 채널을 가진 형태의 데이터를 받도록 설정되어 있으므로 채널을 가지도록 데이터의 shape을 바꿉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "test_x = test_x.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흑백 이미지는 1개의 채널을 갖습니다. reshape()함수로 제일 뒤쪽에 채널 차원을 추가합니다. x1을 곱해준 것이므로 데이터 수는 달라지지 않습니다.\n",
    "> 6000 x 28 x 28 = 6000 x 28 x 28 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컨볼루션 신경망 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 22, 22, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               3965056   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 3,989,642\n",
      "Trainable params: 3,989,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential( [\n",
    "    tf.keras.layers.Conv2D( input_shape=(28,28,1), kernel_size=(3,3), filters=16 ),\n",
    "    tf.keras.layers.Conv2D( kernel_size=(3,3), filters=32 ),\n",
    "    tf.keras.layers.Conv2D( kernel_size=(3,3), filters=64 ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense( units=128, activation='relu' ),\n",
    "    tf.keras.layers.Dense( units=10, activation='softmax' )\n",
    "] )\n",
    "model.compile( optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_9_input to have 4 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6286a428e0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    517\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    518\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_9_input to have 4 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "history = model.fit( train_x, train_y, epochs=25, validation_split=0.25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
