#!/bin/bash
OPTION_FORMAT='mp3'
OPTION_VOICE='Joey'
OPTIONS="--output-format ${OPTION_FORMAT} --voice-id ${OPTION_VOICE}"

TEXT="Slide 10 for 4 minutes. We introduce a reward function that provides a reward for each state. In our example we want to incentivize the agent to follow the centerline because we think that will work best, and so we give higher rewards for actions that result in the agent ending up in the centerline. Why will providing a higher reward in the center incentivize center line driving behavior? Pause. RL algorithms seek to train models that will choose that set of actions, a path so to speak, that maximizes the cumulative expected rewards from any state. Simply put from any state choose the action that will give the highest sum of future rewards. We call this the value function. We also introduce the concept of a discount, to penalize the car for each step taken. Discounting limits how far into the future we look. To build up this value function our agent will explore all state and actions and build a table reflecting the value of being in each state, shown on the right. This value is the maximum cumulative reward achievable from each state when it selects the actions in subsequent states leading to states with highest value. Learning doesnâ€™t just happen in the first go, it takes some iteration because it first needs to explore and see where it can get the highest rewards, before it can exploit that knowledge. How can we improve this? Slide 11 for 2 minutes. We are interested in the creation of a model that can be used by AWS DeepRacer, our agent, to choose which actions to take from any state in our race track environment in order to reach our goal of completing the track. When we have our trained model the only input into the model is the state, the image from the camera, and the output is the action, the speed and direction to our engine and steering. The reward function is only used during training to help optimize the RL model to choose the actions that lead to the maximum cumulative reward. Once the model is trained there is no way to give feedback on how good or bad an action was, not in the simulator and not in the real world."
FILE_NAME="rl4bona-3.mp3"
CMD="aws polly synthesize-speech ${OPTIONS} --text '${TEXT}' ${FILE_NAME}"
echo $CMD
eval $CMD
