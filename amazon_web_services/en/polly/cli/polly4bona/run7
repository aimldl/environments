#!/bin/bash
OPTION_FORMAT='mp3'
OPTION_VOICE='Joey'
OPTIONS="--output-format ${OPTION_FORMAT} --voice-id ${OPTION_VOICE}"

TEXT="Slide 18 for 2 minutes. In the AWS DeepRacer console, 1. Create a model, 2. Configure training. From our reinforcement learning overview we saw that the reward function is a pretty important item when it comes to reinforcement learning. And so we will pay close attention to the reward function. Our reinforcement learning algorithm has certain hyperparameters (such as after how many episodes of experience we update the model) that we need to specify, and we also need to provide the actions from which it should choose. 3. Train the model. 4. Evaluate the model, how good is your model? If you are happy go race in the League (virtual or in person) or deploy to the car. 5. Tweak mode. Slide 19 for 2 minutes. Lets look at the reward function again. During training we take an action in each step and update the position of the car. The reward function will be used to determine how good or bad the outcome of the action is. How would the reward function know if the outcome is good or bad? Pause. You supply the logic for it to determine how good or bad, and quantify it. And you get variables containing measurements from the simulator after each action that you can use to build reward function logic. You do this in python 3 syntax. For example, the simulator tells you how close your car is from the center of the track and how wide the track is so you can give a higher reward for being closer to the center of the track. Letâ€™s look at track components and simulator measurements that can be used for our reward function."
FILE_NAME="rl4bona-7.mp3"
CMD="aws polly synthesize-speech ${OPTIONS} --text '${TEXT}' ${FILE_NAME}"
echo $CMD
eval $CMD
