#!/bin/bash

OPTION_FORMAT='mp3' OPTION_VOICE='Joey' OPTIONS="--output-format ${OPTION_FORMAT} --voice-id ${OPTION_VOICE}"
#TEXT='can i get a wakup call at 8 30 am' FILE_NAME='test3.mp3'
TEXT="Slide 3. Before we get started, let’s do a quick recap on AWS DeepRacer. Slide 4 for 1 minute. Developers told us they love the hands-on learning approach used in DeepLens. In 
2018 Amazon SageMaker added support for Reinforcement Learning. We asked the question “How can we put Reinforcement Learning in the hands of all developers literally?” And so the 
idea of AWS DeepRacer was born. Slide 5 for 2 minutes. Before you tell customers they can order/pre-order the car, check if available in your region!!! Otherwise don’t call it out 
At reinvent 2018 we launched AWS DeepRacer. AWS DeepRacer is a 1/18th scale robotic car which gives you an exciting and fun way to get started with reinforcement learning by 
applying it to autonomous racing. You can order/pre-order your AWS DeepRacer from Amazon today. AWS DeepRacer has a virtual racing simulator that allows you to train, evaluate, and 
iterate on your RL models in a racing environment, quickly and easily. And if you get really good, and want to showcase your machine learning skills in a competitive environment, 
there is the AWS DeepRacer league. You can compete in a global championship - racing your DeepRacer model- for a chance to win several prizes and advance to the AWS DeepRacer 
Championship Cup. Throughout 2019 there will be in person events at selected Aws Summits and a series of virtual competitions hosted in the virtual simulator, giving all developers 
the ability to compete. Slide 7 for 1 minute. Machine Learning has three main categories. Supervised learning. Build a model to predict a value or a classify data. Models are 
trained using large amounts of curated training data, consisting of label and data pairs, to learn. Models learn to accomplish a well defined single task, and don’t scale easily to 
other tasks or sub-tasks without new data. An example is linear regression. Unsupervised learning. Build a model to classify data. Models are trained to identify similarities in 
large amounts of data to aid classification. Training data does not have explicit labels. An example is clustering. Reinforcement learning. Build a model to make autonomous 
decisions in an environment. Models are trained in an environment, by learning from interaction with the environment which decisions/actions led to good outcomes, and which led to 
bad outcomes. You need to supply the environment and very importantly the reward function to help it determine if an action was good or bad. The example is policy optimization. 
Slide 8 for 1 minute. Reinforcement learning is built on and idea that is used quite often, perhaps daily, by humans. When was the last time you used a reward to incentivize the 
right behavior. Think about the method used to train a pet – treats are used to incentivize desired behavior. Training can be for simple actions like sit or stay, to more 
complicated series of behaviors, like you sometimes see in dog agility courses – crawling through tunnels, jumping over hurdles, and weaving through poles. Slide 9 for 2 minutes. 
Like using rewards with pets, we will use a reward function to incentivize the desired driving behavior in our agent. What is a reward function? Simply a function helps your agent 
determine if the action it just took was good or bad and how good or bad. Let’s look at this race grid to see a practical example of a reward function. The blue square is our 
agent’s starting position, green represents its goal. It can move to every other square, except purple, one square at a time. Each square is called a state, and our agent can choose 
from the following actions in each state: move forward, left, or right, always facing in the direction of the goal and only visit each state once. This is an example of a “sparse 
reward function”, it simply says that when the agent reaches the goal it will get a reward of 2. This is like trying to teach your dog to sit, then lie down, then bark, and then 
shake and only then will you reward them. It will probably take a long time to learn this behavior, if at all. Our agent will also take a long time to learn the right behavior as 
there are many paths to get to the goal, and it won’t know which one is better. Slide 10 for 4 minutes. We introduce a reward function that provides a reward for each state. In our 
example we want to incentivize the agent to follow the centerline because we think that will work best, and so we give higher rewards for actions that result in the agent ending up 
in the centerline. Why will providing a higher reward in the center incentivize center line driving behavior? Pause. RL algorithms seek to train models that will choose that set of 
actions, a path so to speak, that maximizes the cumulative expected rewards from any state. Simply put from any state choose the action that will give the highest sum of future 
rewards. We call this the value function. We also introduce the concept of a discount, to penalize the car for each step taken. Discounting limits how far into the future we look. 
To build up this value function our agent will explore all state and actions and build a table reflecting the value of being in each state, shown on the right. This value is the 
maximum cumulative reward achievable from each state when it selects the actions in subsequent states leading to states with highest value. Learning doesn’t just happen in the first 
go, it takes some iteration because it first needs to explore and see where it can get the highest rewards, before it can exploit that knowledge. How can we improve this?" 
FILE_NAME="rl4bona.mp3" CMD="aws polly synthesize-speech ${OPTIONS} --text '${TEXT}' ${FILE_NAME}" echo $CMD eval $CMD
