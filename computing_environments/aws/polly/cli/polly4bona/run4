#!/bin/bash
OPTION_FORMAT='mp3'
OPTION_VOICE='Joey'
OPTIONS="--output-format ${OPTION_FORMAT} --voice-id ${OPTION_VOICE}"

TEXT="Slide 12 for 5 minutes. As in our grid race we saw we have to first explore the environment to know which actions from which states are the best actions. In the simulator AWS DeepRacer will drive around the track and take 15 pictures per second. Each time it takes a picture we refer to as a step. Each picture represents the state it is in, and using an RL model it will inference which action to take. This will lead to a new state (the outcome of the action) and using the reward function it will then calculate the reward based on the outcome. The process will continue in each state, until we get to a terminal state where the car goes off track or finishes the lap. The car will then reset and repeat the process. Each step is a (state, action, new state, reward) tuple and all steps from the reset point till the terminal state is called an episode. You should think of these episodes as experience, or training data for our model. The initial model used to determine which action to take doesn’t know anything. And so our RL algorithm will initially select actions at random. This is so we can explore the environment. After a set number of episodes, we will train our model and improve it using the experience we built up. The improved model will then be used to collect more experience, after which we will train again and get an even better model. We determine how long we will select random actions over the actions determine by our model. If we start exploiting our new found experience too soon, we may end up missing a better path. Similarly if we keep on randomly choosing actions to explore more, we may end up taking a very long time to train a model. If you can explore all state, action combinations you will end up with a value function, like in our grid race. We can then introduce a policy function. A policy function is a mapping from a state to an action. A simple policy once we know the value function is to say always choose the action that will end up in the state with the highest value. So this is easy when you can explore every possible state/action combination – just build a lookup table like in our grid race track example. BUT for many environments, like in AWS DeepRacer, you can’t explore every state/action combination and so you can’t fully determine the value function or a policy function. Two methods we use are Value Approximation and Policy Optimization."
FILE_NAME="rl4bona-4.mp3"
CMD="aws polly synthesize-speech ${OPTIONS} --text '${TEXT}' ${FILE_NAME}"
echo $CMD
eval $CMD
