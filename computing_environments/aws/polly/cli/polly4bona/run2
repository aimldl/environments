#!/bin/bash
OPTION_FORMAT='mp3'
OPTION_VOICE='Joey'
OPTIONS="--output-format ${OPTION_FORMAT} --voice-id ${OPTION_VOICE}"

TEXT="Slide 7 for 1 minute. Machine Learning has three main categories. Supervised learning. Build a model to predict a value or a classify data. Models are trained using large amounts of curated training data, consisting of label and data pairs, to learn. Models learn to accomplish a well defined single task, and don’t scale easily to other tasks or sub-tasks without new data. An example is linear regression. Unsupervised learning. Build a model to classify data. Models are trained to identify similarities in large amounts of data to aid classification. Training data does not have explicit labels. An example is clustering. Reinforcement learning. Build a model to make autonomous decisions in an environment. Models are trained in an environment, by learning from interaction with the environment which decisions/actions led to good outcomes, and which led to bad outcomes. You need to supply the environment and very importantly the reward function to help it determine if an action was good or bad. The example is policy optimization. Slide 8 for 1 minute. Reinforcement learning is built on and idea that is used quite often, perhaps daily, by humans. When was the last time you used a reward to incentivize the right behavior. Think about the method used to train a pet – treats are used to incentivize desired behavior. Training can be for simple actions like sit or stay, to more complicated series of behaviors, like you sometimes see in dog agility courses – crawling through tunnels, jumping over hurdles, and weaving through poles. Slide 9 for 2 minutes. Like using rewards with pets, we will use a reward function to incentivize the desired driving behavior in our agent. What is a reward function? Simply a function helps your agent determine if the action it just took was good or bad and how good or bad. Let’s look at this race grid to see a practical example of a reward function. The blue square is our agent’s starting position, green represents its goal. It can move to every other square, except purple, one square at a time. Each square is called a state, and our agent can choose from the following actions in each state: move forward, left, or right, always facing in the direction of the goal and only visit each state once. This is an example of a “sparse reward function”, it simply says that when the agent reaches the goal it will get a reward of 2. This is like trying to teach your dog to sit, then lie down, then bark, and then shake and only then will you reward them. It will probably take a long time to learn this behavior, if at all. Our agent will also take a long time to learn the right behavior as there are many paths to get to the goal, and it won’t know which one is better."
FILE_NAME="rl4bona-2.mp3"
CMD="aws polly synthesize-speech ${OPTIONS} --text '${TEXT}' ${FILE_NAME}"
echo $CMD
eval $CMD
