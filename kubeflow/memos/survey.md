## Survey Contents

## Fall 2019

[Kubeflow Community User Survey Fall 2019 Results](https://medium.com/kubeflow/kubeflow-community-user-survey-fall-2019-a84776c71743), Elvira Dzhuraeva, Dec 21, 2019 



<img src="https://miro.medium.com/max/1200/0*2YbBJuFBwAYeHErH">

<img src="https://miro.medium.com/max/1200/0*dbTsww8d4FNVjO8G">

> From the scalability standpoint, it is quite clear that bigger organizations require to support more users per cluster for their workloads than smaller organizations.

<img src="https://miro.medium.com/max/1200/0*t1ee3ACRRDjZmgi2">

>  The majority of users expect to run on average 50 notebooks and pipelines on a single cluster simultaneously.

<img src="https://miro.medium.com/max/1200/0*lTb4zNR2veyCZSmb">

From a simple calculation, roughly 5000 models are expected to run simultaneously assuming 100 users run 50 models/user. 



<img src="https://miro.medium.com/max/1200/0*CP6woLPvi9ePwcVW">

<img src="https://miro.medium.com/max/1600/0*BW_ubsPzizfLIu7_">

### Data on the Survey Respondents (Fall 2019)

<img src="https://miro.medium.com/max/1269/0*HelBxrqO0m-h6Fp-">

<img src="https://miro.medium.com/max/1366/0*5aP2xRih_Y_n5yfF">

<img src="https://miro.medium.com/max/1600/0*X23LCSCy-euTkPYd">

<img src="https://miro.medium.com/max/1289/0*ElledoaqR930v_02">

### Spring 2019

[Kubeflow Community User Survey Spring 2019](https://medium.com/kubeflow/kubeflow-community-user-survey-spring-2019-44f86c794e67), Elvira Dzhuraeva, Dec 20, 2019

#### Type of Servers to Run the ML Workloads

<img src="https://miro.medium.com/max/1200/0*ZGVQ1tzvfxW98OSm">

<img src="https://miro.medium.com/max/1200/0*kpsbPyO6vxUppKH4">

<img src="https://miro.medium.com/max/1200/0*WVlBH2Srq2m66Oou">

#### Types of Frameworks Usage

<img src="https://miro.medium.com/max/1600/0*JE7lzaHX-SeWLErh">

<img src="https://miro.medium.com/max/1349/0*a2_Hm2ng7sefW7Kp">

### Data on the Survey Respondents (Spring 2019)

<img src="https://miro.medium.com/max/1412/0*zxg7nbal237hiWA8">

<img src="https://miro.medium.com/max/1200/0*KeqShgjtewGe9O5S">

